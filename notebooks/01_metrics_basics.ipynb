{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Metrics Basics\n",
    "\n",
    "**Comprehensive guide to evaluation metrics for change point detection**\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Point-based metrics (precision, recall, F-beta)\n",
    "2. Distance-based metrics (Hausdorff, annotation error)\n",
    "3. Segmentation metrics (Adjusted Rand Index)\n",
    "4. Multi-annotator metrics (covering metric)\n",
    "5. Visualization techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastcpd.metrics import (\n",
    "    precision_recall,\n",
    "    f_beta_score,\n",
    "    hausdorff_distance,\n",
    "    annotation_error,\n",
    "    adjusted_rand_index,\n",
    "    covering_metric,\n",
    "    evaluate_all\n",
    ")\n",
    "from fastcpd.visualization import plot_detection\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Point-Based Metrics: Precision & Recall\n",
    "\n",
    "Precision and recall are fundamental metrics for evaluating change point detection:\n",
    "\n",
    "- **Precision**: What fraction of detected change points are correct?\n",
    "- **Recall**: What fraction of true change points were detected?\n",
    "\n",
    "A tolerance margin allows \"close enough\" matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 1: Perfect detection\n",
    "true_cps = [100, 200, 300]\n",
    "pred_cps = [100, 200, 300]\n",
    "\n",
    "result = precision_recall(true_cps, pred_cps, margin=10)\n",
    "\n",
    "print(\"Perfect Detection:\")\n",
    "print(f\"  Precision: {result['precision']:.3f}\")\n",
    "print(f\"  Recall:    {result['recall']:.3f}\")\n",
    "print(f\"  F1 Score:  {result['f1_score']:.3f}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  True Positives:  {result['true_positives']}\")\n",
    "print(f\"  False Positives: {result['false_positives']}\")\n",
    "print(f\"  False Negatives: {result['false_negatives']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 2: Detection with small errors (within margin)\n",
    "true_cps = [100, 200, 300]\n",
    "pred_cps = [98, 205, 295]  # All within margin=10\n",
    "\n",
    "result = precision_recall(true_cps, pred_cps, margin=10)\n",
    "\n",
    "print(\"Detection with Small Errors:\")\n",
    "print(f\"  Precision: {result['precision']:.3f}\")\n",
    "print(f\"  Recall:    {result['recall']:.3f}\")\n",
    "print(f\"\\nMatched Pairs (true_cp, pred_cp):\")\n",
    "for true_cp, pred_cp in result['matched_pairs']:\n",
    "    print(f\"  {true_cp} → {pred_cp} (error: {abs(true_cp - pred_cp)})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 3: Over-detection (false positives)\n",
    "true_cps = [100, 200]\n",
    "pred_cps = [100, 150, 200, 250]  # Extra CPs at 150 and 250\n",
    "\n",
    "result = precision_recall(true_cps, pred_cps, margin=10)\n",
    "\n",
    "print(\"Over-Detection (False Positives):\")\n",
    "print(f\"  Precision: {result['precision']:.3f} (2 correct out of 4 predictions)\")\n",
    "print(f\"  Recall:    {result['recall']:.3f} (found all true CPs)\")\n",
    "print(f\"\\nUnmatched Predictions (False Positives):\")\n",
    "print(f\"  {result['unmatched_pred']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 4: Under-detection (false negatives)\n",
    "true_cps = [100, 200, 300, 400]\n",
    "pred_cps = [100, 200]  # Missed 300 and 400\n",
    "\n",
    "result = precision_recall(true_cps, pred_cps, margin=10)\n",
    "\n",
    "print(\"Under-Detection (False Negatives):\")\n",
    "print(f\"  Precision: {result['precision']:.3f} (all predictions correct)\")\n",
    "print(f\"  Recall:    {result['recall']:.3f} (found 2 out of 4)\")\n",
    "print(f\"\\nUnmatched True CPs (False Negatives):\")\n",
    "print(f\"  {result['unmatched_true']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. F-Beta Score: Weighting Precision vs Recall\n",
    "\n",
    "The F-beta score allows you to weight precision and recall differently:\n",
    "\n",
    "- **β = 1** (F1): Equal weight\n",
    "- **β < 1** (F0.5): Favor precision (penalize false positives more)\n",
    "- **β > 1** (F2): Favor recall (penalize false negatives more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: When missing CPs is worse than false alarms\n",
    "true_cps = [100, 200, 300]\n",
    "pred_cps = [100, 200, 250, 300]  # One false positive at 250\n",
    "\n",
    "result = f_beta_score(true_cps, pred_cps, beta=1.0, margin=10)\n",
    "\n",
    "print(\"F-beta Scores:\")\n",
    "print(f\"  F1  (β=1.0): {result['f1_score']:.4f} (balanced)\")\n",
    "print(f\"  F2  (β=2.0): {result['f2_score']:.4f} (favor recall)\")\n",
    "print(f\"  F0.5(β=0.5): {result['f0_5_score']:.4f} (favor precision)\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  Precision: {result['precision']:.3f}\")\n",
    "print(f\"  Recall:    {result['recall']:.3f}\")\n",
    "print(f\"  \\n  F2 > F1: Emphasizing recall increases score when recall is good\")\n",
    "print(f\"  F0.5 < F1: De-emphasizing precision decreases score when precision is lower\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distance-Based Metrics\n",
    "\n",
    "### 3.1 Hausdorff Distance\n",
    "\n",
    "Measures the worst-case distance between two CP sets. Sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "true_cps = [100, 200, 300]\n",
    "pred_cps = [105, 200, 350]  # Errors: 5, 0, 50\n",
    "\n",
    "result = hausdorff_distance(true_cps, pred_cps)\n",
    "\n",
    "print(\"Hausdorff Distance:\")\n",
    "print(f\"  Hausdorff distance: {result['hausdorff']:.1f}\")\n",
    "print(f\"  Forward (true→pred): {result['forward_distance']:.1f}\")\n",
    "print(f\"  Backward (pred→true): {result['backward_distance']:.1f}\")\n",
    "print(f\"\\nClosest Pairs:\")\n",
    "for cp1, cp2, dist in result['closest_pairs']:\n",
    "    print(f\"  {cp1} → {cp2} (distance: {dist:.1f})\")\n",
    "print(f\"\\nNote: Hausdorff = 50 because CP at 300 is 50 away from nearest pred (350)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Annotation Error\n",
    "\n",
    "Measures average localization accuracy using optimal matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "true_cps = [100, 200, 300]\n",
    "pred_cps = [105, 195, 310]  # Errors: 5, 5, 10\n",
    "\n",
    "result_mae = annotation_error(true_cps, pred_cps, method='mae')\n",
    "result_rmse = annotation_error(true_cps, pred_cps, method='rmse')\n",
    "\n",
    "print(\"Annotation Error:\")\n",
    "print(f\"  MAE:  {result_mae['error']:.2f}\")\n",
    "print(f\"  RMSE: {result_rmse['error']:.2f}\")\n",
    "print(f\"  Median: {result_mae['median_error']:.2f}\")\n",
    "print(f\"  Max:    {result_mae['max_error']:.2f}\")\n",
    "print(f\"  Std:    {result_mae['std_error']:.2f}\")\n",
    "print(f\"\\nErrors per CP: {result_mae['errors_per_cp']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentation Metrics: Adjusted Rand Index\n",
    "\n",
    "Measures overall similarity of segmentations, accounting for chance agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perfect segmentation\n",
    "true_cps = [100, 200, 300]\n",
    "pred_cps_perfect = [100, 200, 300]\n",
    "pred_cps_offset = [105, 195, 305]  # Slightly offset\n",
    "\n",
    "result_perfect = adjusted_rand_index(true_cps, pred_cps_perfect, n_samples=400)\n",
    "result_offset = adjusted_rand_index(true_cps, pred_cps_offset, n_samples=400)\n",
    "\n",
    "print(\"Adjusted Rand Index:\")\n",
    "print(f\"\\nPerfect Segmentation:\")\n",
    "print(f\"  ARI: {result_perfect['ari']:.4f}\")\n",
    "print(f\"  Rand Index: {result_perfect['rand_index']:.4f}\")\n",
    "\n",
    "print(f\"\\nSlightly Offset (by 5):\")\n",
    "print(f\"  ARI: {result_offset['ari']:.4f}\")\n",
    "print(f\"  Rand Index: {result_offset['rand_index']:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  ARI = 1.0: Perfect agreement\")\n",
    "print(f\"  ARI = 0.0: Agreement by chance\")\n",
    "print(f\"  ARI < 0.0: Worse than random\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Annotator Metrics: Covering Score\n",
    "\n",
    "**UNIQUE to fastcpd!** Measures how well predictions agree with EACH annotator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulate multiple annotators\n",
    "from fastcpd.datasets import add_annotation_noise\n",
    "\n",
    "true_cps = [100, 200, 300]\n",
    "annotators = add_annotation_noise(true_cps, n_annotators=5, \n",
    "                                   noise_std=5.0, agreement_rate=0.8, seed=42)\n",
    "\n",
    "print(\"Multiple Annotators:\")\n",
    "for i, ann_cps in enumerate(annotators, 1):\n",
    "    print(f\"  Annotator {i}: {ann_cps}\")\n",
    "\n",
    "# Evaluate predictions\n",
    "pred_cps = [100, 200, 300]\n",
    "result = covering_metric(annotators, pred_cps, margin=10)\n",
    "\n",
    "print(f\"\\nCovering Metric Results:\")\n",
    "print(f\"  Covering Score: {result['covering_score']:.3f} (mean recall across annotators)\")\n",
    "print(f\"  Std Recall:     {result['std_recall']:.3f}\")\n",
    "print(f\"  Min Recall:     {result['min_recall']:.3f}\")\n",
    "print(f\"  Max Recall:     {result['max_recall']:.3f}\")\n",
    "print(f\"\\nRecall per Annotator: {[f'{r:.2f}' for r in result['recall_per_annotator']]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Evaluation\n",
    "\n",
    "Use `evaluate_all()` for one-stop evaluation with all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "true_cps = [100, 200, 300]\n",
    "pred_cps = [98, 205, 295]\n",
    "\n",
    "result = evaluate_all(true_cps, pred_cps, n_samples=400, margin=10)\n",
    "\n",
    "# Print formatted summary\n",
    "print(result['summary'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Access individual metrics\n",
    "print(\"\\nPoint Metrics:\")\n",
    "for key, value in result['point_metrics'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nDistance Metrics:\")\n",
    "for key, value in result['distance_metrics'].items():\n",
    "    print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nSegmentation Metrics:\")\n",
    "for key, value in result['segmentation_metrics'].items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization Examples\n",
    "\n",
    "Visualize detection results with metrics overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate sample data\n",
    "from fastcpd.datasets import make_mean_change\n",
    "\n",
    "data_dict = make_mean_change(n_samples=500, n_changepoints=3, \n",
    "                             noise_std=1.0, seed=42)\n",
    "\n",
    "# Simulate detection (add some noise to true CPs)\n",
    "true_cps = data_dict['changepoints']\n",
    "pred_cps = [cp + np.random.randint(-10, 10) for cp in true_cps]\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_all(true_cps, pred_cps, n_samples=500, margin=10)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plot_detection(data_dict['data'], true_cps, pred_cps, \n",
    "                          metric_result=metrics, \n",
    "                          title=\"Mean Change Detection Example\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Tips\n",
    "\n",
    "### Choosing the Right Metric\n",
    "\n",
    "| Use Case | Recommended Metric | Why? |\n",
    "|----------|-------------------|------|\n",
    "| **Binary decision** (detected or not) | Precision/Recall | Clear interpretation |\n",
    "| **False alarms are costly** | F0.5 (β=0.5) | Emphasizes precision |\n",
    "| **Missing CPs is costly** | F2 (β=2.0) | Emphasizes recall |\n",
    "| **Localization accuracy** | Annotation Error | Direct distance measure |\n",
    "| **Worst-case analysis** | Hausdorff | Sensitive to outliers |\n",
    "| **Overall segmentation quality** | Adjusted Rand Index | Accounts for chance |\n",
    "| **Multiple ground truths** | Covering Metric | Evaluates agreement with each |\n",
    "\n",
    "### Choosing the Margin\n",
    "\n",
    "- **Small margin** (1-5): Strict localization\n",
    "- **Medium margin** (10-20): Reasonable tolerance\n",
    "- **Large margin** (50+): Focus on detection, not localization\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always report multiple metrics** - Single metric can be misleading\n",
    "2. **Use evaluate_all()** - Comprehensive evaluation\n",
    "3. **Visualize results** - Plots reveal patterns metrics might miss\n",
    "4. **Consider your application** - Choose metrics that match your goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered:\n",
    "\n",
    "✅ **Point-based metrics**: Precision, Recall, F-beta  \n",
    "✅ **Distance metrics**: Hausdorff, Annotation Error  \n",
    "✅ **Segmentation metrics**: Adjusted Rand Index  \n",
    "✅ **Multi-annotator metrics**: Covering Score (UNIQUE!)  \n",
    "✅ **Comprehensive evaluation**: evaluate_all()  \n",
    "✅ **Visualization**: plot_detection()  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Tutorial 2**: Dataset Generation\n",
    "- **Tutorial 3**: End-to-End Benchmarking\n",
    "\n",
    "---\n",
    "\n",
    "**fastcpd-python** provides the most comprehensive evaluation metrics for change point detection! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
