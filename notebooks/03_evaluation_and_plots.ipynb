{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with fastcpd: Part 3 - Evaluation and Visualization\n",
    "\n",
    "This tutorial shows how to evaluate detection results and create visualizations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcpd import fastcpd\n",
    "from fastcpd.datasets import make_mean_change\n",
    "from fastcpd.metrics import precision_recall, evaluate_all\n",
    "from fastcpd.visualization import plot_detection\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example: Generate → Detect → Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate data\n",
    "data_dict = make_mean_change(n_samples=500, n_changepoints=3, seed=42)\n",
    "data = data_dict['data']\n",
    "true_cps = data_dict['changepoints']\n",
    "\n",
    "# Step 2: Detect change points\n",
    "result = fastcpd(data, family=\"mean\", beta=\"MBIC\")\n",
    "detected_cps = result.cp_set\n",
    "\n",
    "print(\"True change points:    \", true_cps)\n",
    "print(\"Detected change points:\", detected_cps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Evaluation Metrics\n",
    "\n",
    "### Precision and Recall\n",
    "\n",
    "- **Precision**: What fraction of detected CPs are correct?\n",
    "- **Recall**: What fraction of true CPs were found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with tolerance margin of 10 samples\n",
    "metrics = precision_recall(true_cps, detected_cps, margin=10)\n",
    "\n",
    "print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"Recall:    {metrics['recall']:.3f}\")\n",
    "print(f\"F1 Score:  {metrics['f1_score']:.3f}\")\n",
    "print(f\"\\nTrue Positives:  {metrics['true_positives']}\")\n",
    "print(f\"False Positives: {metrics['false_positives']}\")\n",
    "print(f\"False Negatives: {metrics['false_negatives']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation\n",
    "\n",
    "Use `evaluate_all()` to get all metrics at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all metrics\n",
    "all_metrics = evaluate_all(true_cps, detected_cps, n_samples=500, margin=10)\n",
    "\n",
    "# Print summary\n",
    "print(all_metrics['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access individual metric groups\n",
    "print(\"\\nPoint Metrics:\")\n",
    "for key, value in all_metrics['point_metrics'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nDistance Metrics:\")\n",
    "for key, value in all_metrics['distance_metrics'].items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple visualization\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(data, linewidth=0.8, label='Data', color='black', alpha=0.7)\n",
    "\n",
    "# True change points\n",
    "for cp in true_cps:\n",
    "    plt.axvline(cp, color='green', linestyle='--', linewidth=2, alpha=0.7, \n",
    "                label='True CP' if cp == true_cps[0] else '')\n",
    "\n",
    "# Detected change points\n",
    "for cp in detected_cps:\n",
    "    plt.axvline(cp, color='red', linestyle=':', linewidth=2.5, alpha=0.8,\n",
    "                label='Detected CP' if cp == detected_cps[0] else '')\n",
    "\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('Value', fontsize=12)\n",
    "plt.title('Change Point Detection Results', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Visualization with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot with metrics overlay\n",
    "fig, axes = plot_detection(\n",
    "    data, \n",
    "    true_cps, \n",
    "    detected_cps,\n",
    "    metric_result=all_metrics,\n",
    "    title=\"Mean Change Detection with Metrics\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Results\n",
    "\n",
    "### What makes a good detection?\n",
    "\n",
    "**Perfect detection:**\n",
    "- Precision = 1.0 (no false positives)\n",
    "- Recall = 1.0 (no missed change points)\n",
    "- F1 Score = 1.0\n",
    "\n",
    "**Over-detection:**\n",
    "- Precision < 1.0 (too many false alarms)\n",
    "- Recall = 1.0 (found all true CPs)\n",
    "- Many detected CPs\n",
    "\n",
    "**Under-detection:**\n",
    "- Precision = 1.0 (all detections correct)\n",
    "- Recall < 1.0 (missed some true CPs)\n",
    "- Few detected CPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Scenarios\n",
    "\n",
    "### Scenario 1: Perfect Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cps = [100, 200, 300]\n",
    "detected_cps = [100, 200, 300]\n",
    "\n",
    "metrics = precision_recall(true_cps, detected_cps, margin=10)\n",
    "print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"Recall:    {metrics['recall']:.3f}\")\n",
    "print(f\"F1 Score:  {metrics['f1_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Over-detection (False Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cps = [100, 200, 300]\n",
    "detected_cps = [100, 150, 200, 250, 300]  # Extra CPs at 150 and 250\n",
    "\n",
    "metrics = precision_recall(true_cps, detected_cps, margin=10)\n",
    "print(f\"Precision: {metrics['precision']:.3f}  (3 correct out of 5)\")\n",
    "print(f\"Recall:    {metrics['recall']:.3f}  (found all 3 true CPs)\")\n",
    "print(f\"False Positives: {metrics['unmatched_pred']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: Under-detection (Missed CPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cps = [100, 200, 300, 400]\n",
    "detected_cps = [100, 200]  # Missed 300 and 400\n",
    "\n",
    "metrics = precision_recall(true_cps, detected_cps, margin=10)\n",
    "print(f\"Precision: {metrics['precision']:.3f}  (all detections correct)\")\n",
    "print(f\"Recall:    {metrics['recall']:.3f}  (found 2 out of 4)\")\n",
    "print(f\"Missed CPs: {metrics['unmatched_true']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Better Detection\n",
    "\n",
    "### 1. Adjust Beta for Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "data_dict = make_mean_change(n_samples=300, n_changepoints=3, seed=42)\n",
    "data = data_dict['data']\n",
    "true_cps = data_dict['changepoints']\n",
    "\n",
    "# Try different beta values\n",
    "print(\"True change points:\", true_cps)\n",
    "print(\"\\nDetection with different beta values:\")\n",
    "\n",
    "for beta in [\"MBIC\", \"BIC\", 5.0, 15.0]:\n",
    "    result = fastcpd(data, family=\"mean\", beta=beta)\n",
    "    metrics = precision_recall(true_cps, result.cp_set, margin=10)\n",
    "    print(f\"\\nBeta={str(beta):6s}: {len(result.cp_set)} CPs - \"\n",
    "          f\"Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}\")\n",
    "    print(f\"           Detected: {result.cp_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choose Appropriate Margin\n",
    "\n",
    "The margin determines how close a detected CP must be to a true CP to count as correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cps = [100, 200, 300]\n",
    "detected_cps = [105, 195, 305]  # Off by 5 samples\n",
    "\n",
    "for margin in [1, 5, 10, 20]:\n",
    "    metrics = precision_recall(true_cps, detected_cps, margin=margin)\n",
    "    print(f\"Margin={margin:2d}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Quick Evaluation Workflow:\n",
    "\n",
    "```python\n",
    "# 1. Detect\n",
    "result = fastcpd(data, family=\"mean\", beta=\"MBIC\")\n",
    "\n",
    "# 2. Evaluate\n",
    "metrics = precision_recall(true_cps, result.cp_set, margin=10)\n",
    "print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "\n",
    "# 3. Visualize\n",
    "plot_detection(data, true_cps, result.cp_set)\n",
    "```\n",
    "\n",
    "### Key Metrics:\n",
    "- **Precision**: Fraction of detected CPs that are correct\n",
    "- **Recall**: Fraction of true CPs that were found\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### Available Functions:\n",
    "- `precision_recall()` - Basic metrics\n",
    "- `evaluate_all()` - All metrics at once\n",
    "- `plot_detection()` - Visualization with metrics\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "- Try different model families (variance, GLM, ARMA, GARCH)\n",
    "- Experiment with beta values to control sensitivity\n",
    "- Apply to your own real-world data\n",
    "- Explore advanced metrics (Hausdorff distance, Adjusted Rand Index)\n",
    "\n",
    "For more details, see the [documentation](https://github.com/zhangxiany-tamu/fastcpd_Python)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
