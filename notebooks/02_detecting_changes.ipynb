{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with fastcpd: Part 2 - Detecting Change Points\n",
    "\n",
    "This tutorial shows how to use fastcpd to detect change points in time series data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcpd import fastcpd\n",
    "from fastcpd.datasets import make_mean_change, make_glm_change, make_garch_change\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "The `fastcpd()` function detects change points. You need to specify:\n",
    "1. Your data\n",
    "2. The model `family`\n",
    "3. Optional: penalty parameter `beta`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mean Changes\n",
    "\n",
    "Simplest case - detecting when the mean shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate data with STRONG mean changes for clear detection\n# Specify: change points at [100, 200, 250] and large mean shifts (5 std devs)\ndata_dict = make_mean_change(\n    n_samples=300, \n    n_changepoints=3,\n    mean_deltas=[5.0],  # 5 std deviation shifts - very clear signal\n    seed=42\n)\ndata = data_dict['data']\ntrue_cps = data_dict['changepoints']\n\n# Detect change points\nresult = fastcpd(data, family=\"mean\", beta=\"MBIC\")\n\nprint(\"True change points:\", true_cps)\nprint(\"Detected change points:\", result.cp_set)\nprint(f\"SNR: {data_dict['metadata']['snr_db']:.1f} dB (higher is easier to detect)\")\n\n# Visualize\nplt.figure(figsize=(12, 4))\nplt.plot(data, linewidth=0.8, label='Data')\nfor cp in true_cps:\n    plt.axvline(cp, color='green', linestyle='--', alpha=0.6, label='True' if cp == true_cps[0] else '')\nfor cp in result.cp_set:\n    plt.axvline(cp, color='red', linestyle=':', linewidth=2, alpha=0.8, label='Detected' if cp == result.cp_set[0] else '')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Mean Change Detection')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variance Changes\n",
    "\n",
    "Detecting when volatility/variance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fastcpd.datasets import make_variance_change\n\n# Generate data with LARGE variance changes for clear detection\n# Use variance ratios of [1.0, 4.0, 0.5] for strong contrast\ndata_dict = make_variance_change(\n    n_samples=300, \n    n_changepoints=2,\n    variance_ratios=[1.0, 4.0, 0.5],  # Strong variance changes\n    seed=42\n)\ndata = data_dict['data']\ntrue_cps = data_dict['changepoints']\n\n# Detect\nresult = fastcpd(data, family=\"variance\", beta=\"MBIC\")\n\nprint(\"True:\", true_cps)\nprint(\"Detected:\", result.cp_set)\nprint(f\"Variance ratios: {data_dict['metadata']['variance_ratios']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression (with Covariates)\n",
    "\n",
    "When you have predictors/features and want to detect coefficient changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcpd.datasets import make_regression_change\n",
    "\n",
    "# Generate data\n",
    "data_dict = make_regression_change(n_samples=300, n_changepoints=2, n_features=3, seed=42)\n",
    "data = data_dict['data']  # Shape: (n, features+1) - first column is response\n",
    "true_cps = data_dict['changepoints']\n",
    "\n",
    "# Detect\n",
    "result = fastcpd(data, family=\"lm\", beta=\"BIC\")\n",
    "\n",
    "print(\"True:\", true_cps)\n",
    "print(\"Detected:\", result.cp_set)\n",
    "\n",
    "# Visualize response variable\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(data[:, 0], linewidth=0.8)\n",
    "for cp in true_cps:\n",
    "    plt.axvline(cp, color='green', linestyle='--', alpha=0.6)\n",
    "for cp in result.cp_set:\n",
    "    plt.axvline(cp, color='red', linestyle=':', linewidth=2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Response')\n",
    "plt.title('Regression: Coefficient Changes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GLM: Binomial (Binary/Count Data)\n",
    "\n",
    "For logistic regression or binomial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate binary data with STRONG coefficient changes\n# Use larger coefficients for better separation\ndata_dict = make_glm_change(\n    n_samples=300, \n    n_changepoints=2, \n    n_features=3, \n    family='binomial',\n    coef_changes='sign_flip',  # Clear coefficient pattern\n    seed=42\n)\ndata = data_dict['data']\ntrue_cps = data_dict['changepoints']\n\n# Detect - use vanilla_percentage=1.0 (PELT)\nresult = fastcpd(data, family=\"binomial\", beta=\"MBIC\", vanilla_percentage=1.0)\n\nprint(\"True:\", true_cps)\nprint(\"Detected:\", result.cp_set)\nprint(f\"AUC per segment: {[f'{a:.2f}' if a else 'N/A' for a in data_dict['metadata']['separation_per_segment']]}\")\n\n# Visualize\nplt.figure(figsize=(12, 4))\nplt.plot(data[:, 0], 'o', markersize=2, alpha=0.6)\nfor cp in true_cps:\n    plt.axvline(cp, color='green', linestyle='--', alpha=0.6)\nfor cp in result.cp_set:\n    plt.axvline(cp, color='red', linestyle=':', linewidth=2)\nplt.xlabel('Time')\nplt.ylabel('Binary Response')\nplt.title('Binomial GLM Detection')\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "source": "## 5. GLM: Poisson (Count Data)\n\nFor count/rate data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate count data with clear coefficient changes\ndata_dict = make_glm_change(\n    n_samples=300, \n    n_changepoints=2, \n    n_features=3, \n    family='poisson',\n    coef_changes='random',  # Random works better than sign_flip for Poisson\n    seed=42\n)\ndata = data_dict['data']\ntrue_cps = data_dict['changepoints']\n\n# Detect - use vanilla_percentage=1.0 (PELT) for Poisson\nresult = fastcpd(data, family=\"poisson\", beta=\"MBIC\", vanilla_percentage=1.0)\n\nprint(\"True:\", true_cps)\nprint(\"Detected:\", result.cp_set)\nprint(f\"Overdispersion per segment: {[f'{o:.1f}' for o in data_dict['metadata']['overdispersion_per_segment']]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. ARMA Time Series\n\nFor time series with autoregressive and moving average components. We'll use ARMA(1,1) with large coefficient changes for clear detection.\n\n**Note:** ARMA detection can take 20-30 seconds for n=150.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generate ARMA(1,1) data with LARGE coefficient changes\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\nnp.random.seed(42)\ntrue_cps = [50, 100]\n\n# Segment 1: ARMA(1,1) with ar=[0.8], ma=[0.6]\nar1 = np.array([1, -0.8])  # AR polynomial\nma1 = np.array([1, 0.6])   # MA polynomial\narma1 = ArmaProcess(ar1, ma1)\ny1 = arma1.generate_sample(nsample=50, scale=1.0)\n\n# Segment 2: ARMA(1,1) with ar=[-0.7], ma=[-0.5] (BIG change!)\nar2 = np.array([1, 0.7])\nma2 = np.array([1, -0.5])\narma2 = ArmaProcess(ar2, ma2)\ny2 = arma2.generate_sample(nsample=50, scale=1.0)\n\n# Segment 3: ARMA(1,1) with ar=[0.5], ma=[0.3]\nar3 = np.array([1, -0.5])\nma3 = np.array([1, 0.3])\narma3 = ArmaProcess(ar3, ma3)\ny3 = arma3.generate_sample(nsample=50, scale=1.0)\n\ndata = np.concatenate([y1, y2, y3])\n\n# Detect change points using ARMA(1,1) model\n# ARMA models require order parameter [p, q] where p=AR order, q=MA order\nresult = fastcpd(data, family='arma', order=[1, 1], beta='MBIC')\n\nprint(f\"True change points: {true_cps}\")\nprint(f\"Detected change points: {result.cp_set.tolist()}\")\nprint(f\"Coefficients: Seg1=(AR:0.8, MA:0.6), Seg2=(AR:-0.7, MA:-0.5), Seg3=(AR:0.5, MA:0.3)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generate GARCH(1,1) data with EXTREME volatility changes\nfrom arch import arch_model\n\nnp.random.seed(42)\ntrue_cps = [50, 100]\n\n# Segment 1: VERY LOW volatility (omega=0.001, alpha=0.05, beta=0.1)\nmodel1 = arch_model(None, vol='GARCH', p=1, q=1, mean='Zero', rescale=False)\nparams1 = np.array([0.001, 0.05, 0.1])  # [omega, alpha, beta]\nsim1 = model1.simulate(params1, nobs=50)\ny1 = sim1['data'].values\n\n# Segment 2: EXTREMELY HIGH volatility (omega=5.0, alpha=0.4, beta=0.4)\nmodel2 = arch_model(None, vol='GARCH', p=1, q=1, mean='Zero', rescale=False)\nparams2 = np.array([5.0, 0.4, 0.4])\nsim2 = model2.simulate(params2, nobs=50)\ny2 = sim2['data'].values\n\n# Segment 3: MEDIUM volatility (omega=0.1, alpha=0.2, beta=0.3)\nmodel3 = arch_model(None, vol='GARCH', p=1, q=1, mean='Zero', rescale=False)\nparams3 = np.array([0.1, 0.2, 0.3])\nsim3 = model3.simulate(params3, nobs=50)\ny3 = sim3['data'].values\n\ndata = np.concatenate([y1, y2, y3])\n\n# Detect change points using GARCH(1,1) model with MBIC\n# GARCH models require order parameter [p, q] for GARCH(p,q)\nresult = fastcpd(data, family='garch', order=[1, 1], beta='MBIC')\n\nprint(f\"True change points: {true_cps}\")\nprint(f\"Detected change points: {result.cp_set.tolist()}\")\nprint(f\"Volatility: Seg1=VERY LOW (ω=0.001), Seg2=EXTREME (ω=5.0), Seg3=MEDIUM (ω=0.1)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. GARCH Volatility Models\n\nFor detecting changes in volatility/heteroskedasticity. We'll use GARCH(1,1) with extreme volatility changes.\n\n**Note:** GARCH detection can take 20-30 seconds for n=150.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Controlling Data Generation\n\nYou can control signal strength to make changes easier or harder to detect."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Controlling Data Generation\n\nYou can control signal strength to make changes easier or harder to detect:\n\n**Mean changes:**\n```python\n# Weak signal (hard to detect)\ndata_dict = make_mean_change(n_samples=300, n_changepoints=2, mean_deltas=[1.0])\n\n# Strong signal (easy to detect)\ndata_dict = make_mean_change(n_samples=300, n_changepoints=2, mean_deltas=[5.0])\n```\n\n**Variance changes:**\n```python\n# Subtle changes\ndata_dict = make_variance_change(n_samples=300, variance_ratios=[1.0, 1.5, 2.0])\n\n# Strong changes  \ndata_dict = make_variance_change(n_samples=300, variance_ratios=[1.0, 4.0, 0.5])\n```\n\n**GLM (Binomial):**\n```python\n# 'sign_flip' creates clear coefficient pattern (easier to detect)\ndata_dict = make_glm_change(n_samples=300, family='binomial', coef_changes='sign_flip')\n```\n\n**Tip:** Use stronger signals when learning. Once familiar, try weaker signals to test robustness."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate data with STRONG signal\ndata_dict = make_mean_change(n_samples=300, n_changepoints=3, mean_deltas=[5.0], seed=42)\ndata = data_dict['data']\n\n# Try different beta values\nfor beta in [\"MBIC\", \"BIC\", 5.0, 20.0]:\n    result = fastcpd(data, family=\"mean\", beta=beta)\n    beta_str = str(beta) if isinstance(beta, (int, float)) else beta\n    print(f\"Beta={beta_str:>6s}: {len(result.cp_set)} change points detected at {result.cp_set}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Percentage (Speed vs Accuracy)\n",
    "\n",
    "Controls the trade-off between speed and accuracy:\n",
    "- `vanilla_percentage=0.0` - Pure SeGD (faster, approximate)\n",
    "- `vanilla_percentage=1.0` - Pure PELT (slower, exact)\n",
    "- `vanilla_percentage=0.5` - Hybrid (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For small datasets (n < 500): use vanilla_percentage=1.0 for highest accuracy\n",
    "result = fastcpd(data, family=\"mean\", beta=\"MBIC\", vanilla_percentage=1.0)\n",
    "\n",
    "# For larger datasets: use vanilla_percentage=0.5 for speed\n",
    "# result = fastcpd(data, family=\"mean\", beta=\"MBIC\", vanilla_percentage=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n**Model families covered in this tutorial:**\n- `\"mean\"` - Mean changes\n- `\"variance\"` - Variance changes\n- `\"meanvariance\"` - Both mean and variance\n- `\"lm\"` - Linear regression\n- `\"lasso\"` - LASSO regression with L1 regularization\n- `\"binomial\"` - Logistic/binomial GLM\n- `\"poisson\"` - Poisson GLM\n- `\"arma\"` - ARMA time series (requires `order=[p, q]`)\n- `\"garch\"` - GARCH volatility (requires `order=[p, q]`)\n\n**Basic syntax:**\n```python\nresult = fastcpd(data, family=\"mean\", beta=\"MBIC\")\nchange_points = result.cp_set\n```\n\n**Key tips:**\n- Use automatic penalty selection (`beta=\"MBIC\"` or `beta=\"BIC\"`) for reliable results\n- For GLM models (binomial/poisson), use `vanilla_percentage=1.0` for best accuracy\n- ARMA and GARCH require the `order` parameter (e.g., `order=[1, 1]` for ARMA(1,1))\n- Use larger coefficient/parameter changes for more reliable detection\n\n---\n\n**Next**: Part 3 - Evaluating and Visualizing Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fastcpd(data, family=\"mean\", beta=\"MBIC\")\n",
    "\n",
    "print(\"Detected change points:\", result.cp_set)\n",
    "print(\"Number of segments:\", len(result.cp_set) + 1)\n",
    "print(\"\\nAll attributes:\")\n",
    "print(dir(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n**Core model families covered in this tutorial:**\n- `\"mean\"` - Mean changes\n- `\"variance\"` - Variance changes\n- `\"meanvariance\"` - Both mean and variance\n- `\"lm\"` - Linear regression\n- `\"binomial\"` - Logistic/binomial GLM\n\n**Additional families available** (see advanced examples):\n- `\"lasso\"` - LASSO regression\n- `\"poisson\"` - Poisson GLM  \n- `\"garch\"` - GARCH volatility\n- `\"arma\"` - ARMA time series\n\n**Basic syntax:**\n```python\nresult = fastcpd(data, family=\"mean\", beta=\"MBIC\")\nchange_points = result.cp_set\n```\n\n**Best practice**: Use automatic penalty selection (`beta=\"MBIC\"` or `beta=\"BIC\"`) for reliable results.\n\n---\n\n**Next**: Part 3 - Evaluating and Visualizing Results"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}