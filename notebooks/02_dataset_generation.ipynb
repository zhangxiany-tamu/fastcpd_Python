{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Dataset Generation\n",
    "\n",
    "**Comprehensive guide to generating synthetic data for benchmarking**\n",
    "\n",
    "This notebook demonstrates all dataset generators in fastcpd:\n",
    "1. Mean/Variance changes\n",
    "2. Regression changes (Linear & LASSO)\n",
    "3. GLM changes (Binomial & Poisson) - UNIQUE!\n",
    "4. Time series (ARMA & GARCH) - UNIQUE!\n",
    "5. Multi-annotator simulation - UNIQUE!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastcpd.datasets import (\n",
    "    make_mean_change,\n",
    "    make_variance_change,\n",
    "    make_regression_change,\n",
    "    make_glm_change,\n",
    "    make_arma_change,\n",
    "    make_garch_change,\n",
    "    add_annotation_noise\n",
    ")\n",
    "from fastcpd.visualization import plot_dataset_characteristics\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mean Change Detection\n",
    "\n",
    "Generate data with mean shifts. Returns rich metadata including SNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate mean change data\ndata_dict = make_mean_change(\n    n_samples=500,\n    n_changepoints=3,\n    noise_std=1.0,\n    change_type='jump',  # or 'drift' for gradual changes\n    seed=42\n)\n\nprint(\"Mean Change Dataset:\")\nprint(f\"  Data shape: {data_dict['data'].shape}\")\nprint(f\"  Change points: {data_dict['changepoints']}\")\nprint(f\"  True means: {data_dict['true_means']}\")\nprint(f\"\\nMetadata:\")\nprint(f\"  SNR: {data_dict['metadata']['snr_db']:.2f} dB\")\nprint(f\"  Difficulty: {data_dict['metadata']['difficulty']:.3f}\")\nprint(f\"  Segment lengths: {data_dict['metadata']['segment_lengths']}\")\n\n# Visualize\nfig, axes = plot_dataset_characteristics(data_dict, figsize=(12, 8))\nplt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variance Change Detection\n",
    "\n",
    "Generate data with variance/volatility changes. Includes kurtosis analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "data_dict = make_variance_change(\n    n_samples=500,\n    n_changepoints=3,\n    variance_ratios=[1.0, 4.0, 0.5, 2.0],  # Custom variance multipliers\n    change_type='multiplicative',  # or 'additive'\n    seed=42\n)\n\nprint(\"Variance Change Dataset:\")\nprint(f\"  Change points: {data_dict['changepoints']}\")\nprint(f\"  Variance ratios: {data_dict['metadata']['variance_ratios']}\")\nprint(f\"  True variances: {[f'{v:.2f}' for v in data_dict['true_variances']]}\")\nprint(f\"  Kurtosis per segment: {[f'{k:.2f}' for k in data_dict['metadata']['kurtosis_per_segment']]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression Changes\n",
    "\n",
    "Generate linear regression data with coefficient changes. Includes R\u00b2 and condition number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "data_dict = make_regression_change(\n",
    "    n_samples=500,\n",
    "    n_changepoints=3,\n",
    "    n_features=5,\n",
    "    coef_changes='random',  # 'sign_flip', 'magnitude', or custom\n",
    "    correlation=0.3,  # Covariate correlation\n",
    "    noise_std=0.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Regression Change Dataset:\")\n",
    "print(f\"  Data shape: {data_dict['data'].shape} (y + X)\")\n",
    "print(f\"  X shape: {data_dict['X'].shape}\")\n",
    "print(f\"  Change points: {data_dict['changepoints']}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "print(f\"  R\u00b2 per segment: {[f'{r:.3f}' for r in data_dict['metadata']['r_squared_per_segment']]}\")\n",
    "print(f\"  Condition number: {data_dict['metadata']['condition_number']:.2f}\")\n",
    "print(f\"  Effect size: {data_dict['metadata']['effect_size']:.2f}\")\n",
    "print(f\"\\nTrue coefficients (first segment): {data_dict['true_coefs'][0]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GLM Changes - UNIQUE Feature!\n",
    "\n",
    "### 4.1 Binomial (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "data_dict = make_glm_change(\n",
    "    n_samples=500,\n",
    "    n_changepoints=3,\n",
    "    n_features=3,\n",
    "    family='binomial',\n",
    "    coef_changes='random',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Binomial GLM Dataset:\")\n",
    "print(f\"  y shape: {data_dict['y'].shape}\")\n",
    "print(f\"  y values: {np.unique(data_dict['y'])}\")\n",
    "print(f\"  Change points: {data_dict['changepoints']}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "print(f\"  Family: {data_dict['metadata']['family']}\")\n",
    "print(f\"  Separation (AUC) per segment: {[f'{s:.3f}' if s else 'N/A' for s in data_dict['metadata']['separation_per_segment']]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Poisson (Count Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "data_dict = make_glm_change(\n",
    "    n_samples=500,\n",
    "    n_changepoints=3,\n",
    "    n_features=3,\n",
    "    family='poisson',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Poisson GLM Dataset:\")\n",
    "print(f\"  y range: [{data_dict['y'].min()}, {data_dict['y'].max()}]\")\n",
    "print(f\"  Change points: {data_dict['changepoints']}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "print(f\"  Overdispersion per segment: {[f'{od:.2f}' for od in data_dict['metadata']['overdispersion_per_segment']]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Series - ARMA - UNIQUE Feature!\n",
    "\n",
    "Generate ARMA processes with parameter changes. Includes stationarity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "data_dict = make_arma_change(\n",
    "    n_samples=500,\n",
    "    n_changepoints=3,\n",
    "    orders=[(1,1), (2,0), (0,2), (1,1)],  # ARMA orders per segment\n",
    "    innovation='normal',  # 't', 'skew_normal'\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"ARMA Dataset:\")\n",
    "print(f\"  Data shape: {data_dict['data'].shape}\")\n",
    "print(f\"  Change points: {data_dict['changepoints']}\")\n",
    "print(f\"  Orders: {data_dict['metadata']['orders']}\")\n",
    "print(f\"\\nStationarity checks:\")\n",
    "print(f\"  Is stationary: {data_dict['metadata']['is_stationary']}\")\n",
    "print(f\"  Is invertible: {data_dict['metadata']['is_invertible']}\")\n",
    "print(f\"\\nTrue parameters (segment 1):\")\n",
    "print(f\"  AR coefs: {data_dict['true_params'][0]['ar']}\")\n",
    "print(f\"  MA coefs: {data_dict['true_params'][0]['ma']}\")\n",
    "print(f\"  Sigma: {data_dict['true_params'][0]['sigma']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series - GARCH - UNIQUE Feature!\n",
    "\n",
    "Generate GARCH processes with volatility regime changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "data_dict = make_garch_change(\n",
    "    n_samples=600,\n",
    "    n_changepoints=2,\n",
    "    volatility_regimes=['low', 'high', 'low'],  # Predefined regimes\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"GARCH Dataset:\")\n",
    "print(f\"  Returns shape: {data_dict['data'].shape}\")\n",
    "print(f\"  Volatility shape: {data_dict['volatility'].shape}\")\n",
    "print(f\"  Change points: {data_dict['changepoints']}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "print(f\"  Volatility regimes: {data_dict['metadata']['volatility_regimes']}\")\n",
    "print(f\"  Avg volatility per segment: {[f'{v:.3f}' for v in data_dict['metadata']['avg_volatility_per_segment']]}\")\n",
    "print(f\"  Volatility ratios: {[f'{r:.2f}' for r in data_dict['metadata']['volatility_ratios']]}\")\n",
    "print(f\"  Kurtosis per segment: {[f'{k:.2f}' for k in data_dict['metadata']['kurtosis_per_segment']]}\")\n",
    "\n",
    "# Plot returns and volatility\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "ax1.plot(data_dict['data'], linewidth=0.5)\n",
    "ax1.set_ylabel('Returns')\n",
    "ax1.set_title('GARCH Returns with Volatility Regime Changes')\n",
    "for cp in data_dict['changepoints']:\n",
    "    ax1.axvline(cp, color='r', linestyle='--', alpha=0.7)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(data_dict['volatility'], color='orange', linewidth=1)\n",
    "ax2.set_ylabel('Conditional Volatility')\n",
    "ax2.set_xlabel('Time')\n",
    "for cp in data_dict['changepoints']:\n",
    "    ax2.axvline(cp, color='r', linestyle='--', alpha=0.7)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Annotator Simulation - UNIQUE Feature!\n",
    "\n",
    "Simulate multiple human annotators with varying agreement levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from fastcpd.visualization import plot_annotators\n",
    "\n",
    "# True change points\n",
    "true_cps = [100, 200, 300]\n",
    "\n",
    "# Simulate 5 annotators\n",
    "annotators = add_annotation_noise(\n",
    "    true_changepoints=true_cps,\n",
    "    n_annotators=5,\n",
    "    noise_std=5.0,  # Location variability\n",
    "    agreement_rate=0.8,  # Probability of including each CP\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Multi-Annotator Simulation:\")\n",
    "print(f\"  True CPs: {true_cps}\\n\")\n",
    "for i, ann_cps in enumerate(annotators, 1):\n",
    "    print(f\"  Annotator {i}: {ann_cps}\")\n",
    "\n",
    "# Generate sample data for visualization\n",
    "data = np.concatenate([\n",
    "    np.random.normal(0, 1, 100),\n",
    "    np.random.normal(5, 1, 100),\n",
    "    np.random.normal(2, 1, 100),\n",
    "    np.random.normal(-2, 1, 100)\n",
    "])\n",
    "\n",
    "# Simulated algorithm prediction\n",
    "pred_cps = [98, 205, 295]\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plot_annotators(data, annotators, pred_cps, figsize=(14, 6))\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset Comparison Table\n",
    "\n",
    "| Dataset | fastcpd | ruptures | Unique Features |\n",
    "|---------|---------|----------|----------------|\n",
    "| Mean Change | \u2705 | \u2705 | SNR, difficulty score, jump/drift |\n",
    "| Variance Change | \u2705 | \u2705 | Kurtosis, multiplicative/additive |\n",
    "| Regression | \u2705 | \u2705 | R\u00b2, condition number, correlation |\n",
    "| **GLM (Binomial)** | \u2705 | \u274c | **AUC per segment** \ud83c\udf1f |\n",
    "| **GLM (Poisson)** | \u2705 | \u274c | **Overdispersion** \ud83c\udf1f |\n",
    "| ARMA | \u2705 | \u26a0\ufe0f | **Stationarity checks** \ud83c\udf1f |\n",
    "| **GARCH** | \u2705 | \u274c | **Volatility tracking** \ud83c\udf1f |\n",
    "| **Multi-Annotator** | \u2705 | \u274c | **Annotation simulation** \ud83c\udf1f |\n",
    "\n",
    "**fastcpd provides 3 UNIQUE dataset types not available in ruptures!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "### Choosing Dataset Parameters\n",
    "\n",
    "1. **SNR for difficulty control**:\n",
    "   - High SNR (>10 dB): Easy detection\n",
    "   - Medium SNR (0-10 dB): Moderate difficulty\n",
    "   - Low SNR (<0 dB): Challenging\n",
    "\n",
    "2. **Segment lengths**:\n",
    "   - Too short: Difficult to estimate parameters\n",
    "   - Too long: Easy to detect\n",
    "   - Balanced: n/(n_cp+1) \u2248 50-200\n",
    "\n",
    "3. **Reproducibility**:\n",
    "   - Always set `seed` for reproducible experiments\n",
    "   - Document all parameters\n",
    "\n",
    "4. **Metadata usage**:\n",
    "   - Use SNR/R\u00b2/AUC to assess dataset quality\n",
    "   - Check stationarity for time series\n",
    "   - Verify condition number for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered:\n",
    "\n",
    "\u2705 **Core datasets**: Mean, Variance, Regression  \n",
    "\u2705 **GLM datasets**: Binomial, Poisson (UNIQUE!)  \n",
    "\u2705 **Time series**: ARMA, GARCH (UNIQUE!)  \n",
    "\u2705 **Multi-annotator**: Simulation (UNIQUE!)  \n",
    "\u2705 **Rich metadata**: SNR, R\u00b2, AUC, stationarity  \n",
    "\u2705 **Visualization**: Dataset characteristics  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Tutorial 3**: End-to-End Benchmarking\n",
    "- Combine datasets with fastcpd detection\n",
    "- Evaluate using comprehensive metrics\n",
    "\n",
    "---\n",
    "\n",
    "**fastcpd-python** provides the most comprehensive dataset generation for change point research! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}